---
title: 'Behavior Design: Improving GOAP with Continuous Refinement Functions'
status: draft
tags:
    - design
    - AI
    - Systems for Systems
    - Magic
    - More Magic
    - Even More Magic
---

import Callout from '@c/md/Callout.astro';

# Behavior Design: Improving GOAP with Continuous Refinement Functions

<Callout kind="summary">
    Goal Oriented Action Planning gives us a robust way to tackle the practical side of behavior design, but it doesn't give us many tools for making behavior feel "organic."
    In order to do that, you need to insert flaws into the behavior loop, ideally in a way that exposes points of control for your design team.
    
    And that's where Continuous Refinement Functions come in.

    CRFs are coroutine-ish functions that return successively higher-quality results with each yield.
    They're most easily used in perception systems (e.g. a Sound Perception function that returns a successively narrower conic section for determining the location of an audio source), but they really shine when you use them throughout the entire behavior model.
    
    Why?

    Because CRFs are inherently responsive to pressure.
    When you use them throughout your behavior system, your designers can tune playstyle-specific reactivity.
    You can reward aggression, encourage stealth, make a robot feel inherently different than a human, all by parameterizing pressure responses.

    So let's learn how to use them.
</Callout>

One of my favorite things to see in games are enemies that are aggressive _until you land a headshot._
It's a form of reactivity that rewards skillful play and it can lead to some very interesting feedback loops.

My preferred way to _implement_ that kind of reactivity is to insert Continuous Refinement Functions into all the places where behavior systems produce **intentionally non-optimal results.**

But what's the best way to build that kind of behavior?

## Reactivity

### _Fake_ Reactivity

Naive approaches to behavior design generally boil down to an `decision` → `delay` → `action` → `delay` loop,
where entities have nigh-perfect perception, single-frame decision making, and series of non-optimal behaviors to pick from semi-randomly.

Naive approaches involve lots of behavior _switching,_ but they can't really make good or bad decisions; it's all hard thresholds, booleans, and enum checks, with some ham-fisted randomness thrown in.
There's no _interpretation_ happening.

You can, of course, manually layer in vaguely-human-feeling behavior, but in naive approaches that tends to mean a proliferation of booleans and hard branching.
If you're just grafting in something simple like headshot reactivity, it isn't too bad, but it can quickly become unmaintainable.

### _Human_ Reactivity

The average human reaction time (to visual cues) falls somewhere in the 150 to 300 millisecond range.
Let's call that 9 to 18 frames at 60fps, just to keep the numbers simple.

In simple perceive-intercept tasks (e.g. catching a falling object that is within arm's reach), we tend to need 400 to 600 milliseconds of observation time.
[Read This](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5501917/) [and this](https://www.frontiersin.org/articles/10.3389/fphys.2023.1266332/full).
Lets call that 24 to 36 frames at 60fps.

The more background stimuli there are, the slower we respond.
Total Perception Reaction Time (e.g. the full perceive → observe → react loop) in distracting real world environments can often end up in the ballpark of 1000 to 2000 milliseconds.
60 to 120 frames at 60fps.

And all of that is for relatively simple motor control tasks.
Catching things, pointing at things, pressing buttons; there's minimal _conscious_ branching here.

Similarly, these numbers don't account for moving targets, interruptions, or other external pressures.
These numbers give us a ballpark for average-minimums, but they don't give us relationships between observation times and accuracy.

We shouldn't get lost in the human side of it, though — how do we account for those factors within the context of behavior design?

#### Pretending You're Human (The Right Way)

A `perception-decision` → `delay` → `action` → `delay` loop can emulate the Perception Reaction Time of a human in a lab environment, but it doesn't handle environmental pressure and interruptions very well.

What we need instead is a `perception over time` → `decision over time` → `action over time` loop, where each step relies on interruptible estimate-refining functions.
With this approach, changing the time given to each step changes the accuracy of their results.
It's inherently reactive to pressure.

<Callout kind="aside">
    ##### Isn't That Computationally Intense?

    Yes. It is.

    Thankfully, the biggest performance optimization you can do — reducing pointer indirection by copying values into pre-allocated spans/arrays and operating on slightly-stale data — aligns with the weaknesses of human perception.

    Your hot loop can be pretty darn hot, without much of a performance impact, when you use the right data structures and allocation strategies.

</Callout>

Using this interruptible estimate-refining pattern as your base, you can build all sorts of "human-like" patterns into your behavior system.

## Decision Making

### Fake Decision Making

Naive decision making tends to rely on hard branches, selectively softened by biased random pick functions.

### Human Decision Making

A highly simplified model of human decision making tends to consist of:

1. Scenario Recognition
2. Action Enumeration
3. Action Cost Assessment

-   Failure Cost Assessment
-   Success Value Assessment
-   Action Selection

7. Action Execution
